{
    "docs": [
        {
            "location": "/", 
            "text": "ProximalOperators.jl\n\n\nProximalOperators is a \nJulia\n package that implements first-order primitives for a variety of functions, which are commonly used for implementing optimization algorithms in several application areas, \ne.g.\n, statistical learning, image and signal processing, optimal control.\n\n\nPlease refer to the \nGitHub repository\n to browse the source code, report issues and submit pull requests.\n\n\n\n\nInstallation\n\n\nTo install the package, use the following in the Julia command line\n\n\nPkg.add(\nProximalOperators\n)\n\n\n\n\nTo load the package simply type\n\n\nusing ProximalOperators\n\n\n\n\nRemember to do \nPkg.update()\n from time to time, to keep the package up to date.\n\n\n\n\nQuick introduction\n\n\nFor a function \nf\n and a stepsize \n\\gamma > 0\n, the \nproximal operator\n (or \nproximal mapping\n) is given by\n\n\n\n\n\n\\mathrm{prox}_{\\gamma f}(x) = \\arg\\min_z \\left\\{ f(z) + \\tfrac{1}{2\\gamma}\\|z-x\\|^2 \\right\\}\n\n\n\n\n\nand can be efficiently computed for many functions \nf\n used in applications.\n\n\nProximalOperators allows to pick function \nf\n from a \nlibrary of commonly used functions\n, and to modify and combine them using \ncalculus rules\n to obtain new ones. The proximal mapping of \nf\n is then provided through the \nprox\n and \nprox!\n methods, as described \nhere\n.\n\n\nFor example, one can create the L1-norm as follows.\n\n\njulia\n using ProximalOperators\n\njulia\n f = NormL1(3.5)\ndescription : weighted L1 norm\ndomain      : AbstractArray{Real}, AbstractArray{Complex}\nexpression  : x \u21a6 \u03bb||x||_1\nparameters  : \u03bb = 3.5\n\n\n\n\nFunctions created this way are, of course, callable.\n\n\njulia\n x = [1.0, 2.0, 3.0, 4.0, 5.0]; # some point\n\njulia\n f(x)\n52.5\n\n\n\n\nMethod \nprox\n evaluates the proximal operator associated with a function, given a point and (optionally) a positive stepsize parameter, returning the proximal point \ny\n and the value of the function at \ny\n:\n\n\njulia\n y, fy = prox(f, x, 0.5) # last argument is 1.0 if absent\n([0.0, 0.25, 1.25, 2.25, 3.25], 24.5)\n\n\n\n\nMethod \nprox!\n evaluates the proximal operator \nin place\n, and only returns the function value at the proximal point (in this case \ny\n must be preallocated and have the same shape/size as \nx\n):\n\n\njulia\n y = similar(x); # allocate y\n\njulia\n fy = prox!(y, f, x, 0.5) # in-place equivalent to y, fy = prox(f, x, 0.5)\n24.5\n\n\n\n\n\n\nBibliographic references\n\n\n\n\nN. Parikh and S. Boyd (2014), \nProximal Algorithms\n, Foundations and Trends in Optimization, vol. 1, no. 3, pp. 127-239.\n\n\nS. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein (2011), \nDistributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers\n, Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1-122.\n\n\n\n\n\n\nCredits\n\n\nProximalOperators.jl is developed by \nLorenzo Stella\n and \nNiccol\u00f2 Antonello\n at \nKU Leuven, ESAT/Stadius\n, and \nMattias F\u00e4lt\n at \nLunds Universitet, Department of Automatic Control\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#proximaloperatorsjl", 
            "text": "ProximalOperators is a  Julia  package that implements first-order primitives for a variety of functions, which are commonly used for implementing optimization algorithms in several application areas,  e.g. , statistical learning, image and signal processing, optimal control.  Please refer to the  GitHub repository  to browse the source code, report issues and submit pull requests.", 
            "title": "ProximalOperators.jl"
        }, 
        {
            "location": "/#installation", 
            "text": "To install the package, use the following in the Julia command line  Pkg.add( ProximalOperators )  To load the package simply type  using ProximalOperators  Remember to do  Pkg.update()  from time to time, to keep the package up to date.", 
            "title": "Installation"
        }, 
        {
            "location": "/#quick-introduction", 
            "text": "For a function  f  and a stepsize  \\gamma > 0 , the  proximal operator  (or  proximal mapping ) is given by   \n\\mathrm{prox}_{\\gamma f}(x) = \\arg\\min_z \\left\\{ f(z) + \\tfrac{1}{2\\gamma}\\|z-x\\|^2 \\right\\}   and can be efficiently computed for many functions  f  used in applications.  ProximalOperators allows to pick function  f  from a  library of commonly used functions , and to modify and combine them using  calculus rules  to obtain new ones. The proximal mapping of  f  is then provided through the  prox  and  prox!  methods, as described  here .  For example, one can create the L1-norm as follows.  julia  using ProximalOperators\n\njulia  f = NormL1(3.5)\ndescription : weighted L1 norm\ndomain      : AbstractArray{Real}, AbstractArray{Complex}\nexpression  : x \u21a6 \u03bb||x||_1\nparameters  : \u03bb = 3.5  Functions created this way are, of course, callable.  julia  x = [1.0, 2.0, 3.0, 4.0, 5.0]; # some point\n\njulia  f(x)\n52.5  Method  prox  evaluates the proximal operator associated with a function, given a point and (optionally) a positive stepsize parameter, returning the proximal point  y  and the value of the function at  y :  julia  y, fy = prox(f, x, 0.5) # last argument is 1.0 if absent\n([0.0, 0.25, 1.25, 2.25, 3.25], 24.5)  Method  prox!  evaluates the proximal operator  in place , and only returns the function value at the proximal point (in this case  y  must be preallocated and have the same shape/size as  x ):  julia  y = similar(x); # allocate y\n\njulia  fy = prox!(y, f, x, 0.5) # in-place equivalent to y, fy = prox(f, x, 0.5)\n24.5", 
            "title": "Quick introduction"
        }, 
        {
            "location": "/#bibliographic-references", 
            "text": "N. Parikh and S. Boyd (2014),  Proximal Algorithms , Foundations and Trends in Optimization, vol. 1, no. 3, pp. 127-239.  S. Boyd, N. Parikh, E. Chu, B. Peleato and J. Eckstein (2011),  Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers , Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1-122.", 
            "title": "Bibliographic references"
        }, 
        {
            "location": "/#credits", 
            "text": "ProximalOperators.jl is developed by  Lorenzo Stella  and  Niccol\u00f2 Antonello  at  KU Leuven, ESAT/Stadius , and  Mattias F\u00e4lt  at  Lunds Universitet, Department of Automatic Control .", 
            "title": "Credits"
        }, 
        {
            "location": "/functions/", 
            "text": "Functions\n\n\nHere we list the available functions, grouped by category. Each function is documented with its exact definition and the necessary parameters for construction. The proximal mapping (and gradient, when defined) of such functions is computed by calling the \nprox\n and \nprox!\n methods (and \ngradient\n, \ngradient!\n, when defined). These functions can be modified and/or combined together to make new ones, by means of \ncalculus rules\n.\n\n\n\n\nIndicators of sets\n\n\nWhen function \nf\n is the indicator function of a set \nS\n, that is\n\n\n\n\n\nf(x) = \u03b4_S(x) =\n\\begin{cases}\n0 & \\text{if}\\ x \\in S, \\\\\n+\u221e & \\text{otherwise},\n\\end{cases}\n\n\n\n\n\nthen \n\\mathrm{prox}_{\u03b3f} = \u03a0_S\n is the projection onto \nS\n. Therefore ProximalOperators includes in particular projections onto commonly used sets, which are here listed.\n\n\n#\n\n\nProximalOperators.IndAffine\n \n \nType\n.\n\n\nIndicator of an affine subspace\n\n\nIndAffine(A, b)\n\n\n\n\nIf \nA\n is a matrix (dense or sparse) and \nb\n is a vector, returns the indicator function of the set\n\n\n\n\n\nS = \\{x : Ax = b\\}.\n\n\n\n\n\nIf \nA\n is a vector and \nb\n is a scalar, returns the indicator function of the set\n\n\n\n\n\nS = \\{x : \\langle A, x \\rangle = b\\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndBallLinf\n \n \nFunction\n.\n\n\nIndicator of a $L_\u221e$ norm ball\n\n\nIndBallLinf(r=1.0)\n\n\n\n\nReturns the indicator function of the set\n\n\n\n\n\nS = \\{ x : \\max (|x_i|) \\leq r \\}.\n\n\n\n\n\nParameter \nr\n must be positive.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndBallL0\n \n \nType\n.\n\n\nIndicator of a $L_0$ pseudo-norm ball\n\n\nIndBallL0(r=1)\n\n\n\n\nReturns the indicator function of the set\n\n\n\n\n\nS = \\{ x : \\mathrm{nnz}(x) \\leq r \\}.\n\n\n\n\n\nParameter \nr\n must be a positive integer.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndBallL1\n \n \nType\n.\n\n\nIndicator of a $L_1$ norm ball\n\n\nIndBallL1(r=1.0)\n\n\n\n\nReturns the indicator function of the set\n\n\n\n\n\nS = \\left\\{ x : \u2211_i |x_i| \\leq r \\right\\}.\n\n\n\n\n\nParameter \nr\n must be positive.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndBallL2\n \n \nType\n.\n\n\nIndicator of a Euclidean ball\n\n\nIndBallL2(r=1.0)\n\n\n\n\nReturns the indicator function of the set\n\n\n\n\n\nS = \\{ x : \\|x\\| \\leq r \\},\n\n\n\n\n\nwhere $|\\cdot|$ is the $L_2$ (Euclidean) norm. Parameter \nr\n must be positive.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndBallRank\n \n \nType\n.\n\n\nIndicator of rank ball\n\n\nIndBallRank(r=1)\n\n\n\n\nReturns the indicator function of the set of matrices of rank at most \nr\n:\n\n\n\n\n\nS = \\{ X : \\mathrm{rank}(X) \\leq r \\},\n\n\n\n\n\nParameter \nr\n must be a positive integer.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndBinary\n \n \nType\n.\n\n\nIndicator of the product of binary sets\n\n\nIndBinary(low, up)\n\n\n\n\nReturns the indicator function of the set\n\n\n\n\n\nS = \\{ x : x_i = low_i\\ \\text{or}\\ x_i = up_i \\},\n\n\n\n\n\nParameters \nlow\n and \nup\n can be either scalars or arrays of the same dimension as the space.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndBox\n \n \nType\n.\n\n\nIndicator of a box\n\n\nIndBox(low, up)\n\n\n\n\nReturns the indicator function of the set\n\n\n\n\n\nS = \\{ x : low \\leq x \\leq up \\}.\n\n\n\n\n\nParameters \nlow\n and \nup\n can be either scalars or arrays of the same dimension as the space: they must satisfy \nlow \n= up\n, and are allowed to take values \n-Inf\n and \n+Inf\n to indicate unbounded coordinates.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndHalfspace\n \n \nType\n.\n\n\nIndicator of a halfspace\n\n\nIndHalfspace(a, b)\n\n\n\n\nFor an array \na\n and a scalar \nb\n, returns the indicator of set\n\n\n\n\n\nS = \\{x : \\langle a,x \\rangle \\leq b \\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndPoint\n \n \nType\n.\n\n\nIndicator of a singleton\n\n\nIndPoint(p=0.0)\n\n\n\n\nReturns the indicator of the set\n\n\n\n\n\nC = \\{p \\}.\n\n\n\n\n\nParameter \np\n can be a scalar, in which case the unique element of \nS\n has uniform coefficients.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndSimplex\n \n \nType\n.\n\n\nIndicator of a simplex\n\n\nIndSimplex(a=1.0)\n\n\n\n\nReturns the indicator of the set\n\n\n\n\n\nS = \\left\\{ x : x \\geq 0, \u2211_i x_i = a \\right\\}.\n\n\n\n\n\nBy default \na=1.0\n, therefore $S$ is the probability simplex.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndSphereL2\n \n \nType\n.\n\n\nIndicator of a Euclidean sphere\n\n\nIndSphereL2(r=1.0)\n\n\n\n\nReturns the indicator function of the set\n\n\n\n\n\nS = \\{ x : \\|x\\| = r \\},\n\n\n\n\n\nwhere $|cdot|$ is the $L_2$ (Euclidean) norm. Parameter \nr\n must be positive.\n\n\nsource\n\n\n\n\nIndicators of convex cones\n\n\nAn important class of sets in optimization is that of convex cones. These are used in particular for formulating \ncone programming problems\n, a family of problems which includes linear programs (LP), quadratic programs (QP), quadratically constrained quadratic programs (QCQP) and semidefinite programs (SDP).\n\n\n#\n\n\nProximalOperators.IndExpPrimal\n \n \nType\n.\n\n\nIndicator of the (primal) exponential cone\n\n\nIndExpPrimal()\n\n\n\n\nReturns the indicator function of the primal exponential cone, that is\n\n\n\n\n\nC = \\mathrm{cl} \\{ (r,s,t) : s > 0, s\u22c5e^{r/s} \\leq t \\} \\subset \\mathbb{R}^3.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndExpDual\n \n \nFunction\n.\n\n\nIndicator of the (dual) exponential cone\n\n\nIndExpDual()\n\n\n\n\nReturns the indicator function of the dual exponential cone, that is\n\n\n\n\n\nC = \\mathrm{cl} \\{ (u,v,w) : u < 0, -u\u22c5e^{v/u} \\leq w\u22c5e \\} \\subset \\mathbb{R}^3.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndFree\n \n \nType\n.\n\n\nIndicator of the free cone\n\n\nIndFree()\n\n\n\n\nReturns the indicator function of the whole space, or \"free cone\", \ni.e.\n, a function which is identically zero.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndNonnegative\n \n \nType\n.\n\n\nIndicator of the nonnegative orthant\n\n\nIndNonnegative()\n\n\n\n\nReturns the indicator of the set\n\n\n\n\n\nC = \\{ x : x \\geq 0 \\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndNonpositive\n \n \nType\n.\n\n\nIndicator of the nonpositive orthant\n\n\nIndNonpositive()\n\n\n\n\nReturns the indicator of the set\n\n\n\n\n\nC = \\{ x : x \\leq 0 \\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndPSD\n \n \nType\n.\n\n\nIndicator of the set of positive semi-definite cone\n\n\nIndPSD()\n\n\n\n\nReturns the indicator of the set\n\n\n\n\n\nC = \\{ X : X \\succeq 0 \\}.\n\n\n\n\n\nThe argument to the function can be either a \nSymmetric\n or \nHermitian\n object, or an object of type \nAbstractVector{Float64}\n holding a symmetric matrix in (lower triangular) packed storage.\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndSOC\n \n \nType\n.\n\n\nIndicator of the second-order cone\n\n\nIndSOC()\n\n\n\n\nReturns the indicator of the second-order cone (also known as ice-cream cone or Lorentz cone), that is\n\n\n\n\n\nC = \\left\\{ (t, x) : \\|x\\| \\leq t \\right\\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndRotatedSOC\n \n \nType\n.\n\n\nIndicator of the rotated second-order cone\n\n\nIndRotatedSOC()\n\n\n\n\nReturns the indicator of the \nrotated\n second-order cone (also known as ice-cream cone or Lorentz cone), that is\n\n\n\n\n\nC = \\left\\{ (p, q, x) : \\|x\\|^2 \\leq 2\\cdot pq, p \\geq 0, q \\geq 0 \\right\\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.IndZero\n \n \nType\n.\n\n\nIndicator of the zero cone\n\n\nIndZero()\n\n\n\n\nReturns the indicator function of the set containing the origin, the \"zero cone\".\n\n\nsource\n\n\n\n\nNorms and regularization functions\n\n\n#\n\n\nProximalOperators.ElasticNet\n \n \nType\n.\n\n\nElastic-net regularization\n\n\nElasticNet(\u03bc=1.0, \u03bb=1.0)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(x) = \u03bc\\|x\\|_1 + (\u03bb/2)\\|x\\|^2,\n\n\n\n\n\nfor nonnegative parameters \n\u03bc\n and \n\u03bb\n.\n\n\nsource\n\n\n#\n\n\nProximalOperators.NormL0\n \n \nType\n.\n\n\n$L_0$ pseudo-norm\n\n\nNormL0(\u03bb=1.0)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(x) = \u03bb\\cdot\\mathrm{nnz}(x)\n\n\n\n\n\nfor a nonnegative parameter \n\u03bb\n.\n\n\nsource\n\n\n#\n\n\nProximalOperators.NormL1\n \n \nType\n.\n\n\n$L_1$ norm\n\n\nNormL1(\u03bb=1.0)\n\n\n\n\nWith a nonnegative scalar parameter \u03bb, returns the function\n\n\n\n\n\nf(x) = \u03bb\\cdot\u2211_i|x_i|.\n\n\n\n\n\nWith a nonnegative array parameter \u03bb, returns the function\n\n\n\n\n\nf(x) = \u2211_i \u03bb_i|x_i|.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.NormL2\n \n \nType\n.\n\n\n$L_2$ norm\n\n\nNormL2(\u03bb=1.0)\n\n\n\n\nWith a nonnegative scalar parameter \u03bb, returns the function\n\n\n\n\n\nf(x) = \u03bb\\cdot\\sqrt{x_1^2 + \u2026 + x_n^2}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.NormL21\n \n \nType\n.\n\n\nSum-of-$L_2$ norms\n\n\nNormL21(\u03bb=1.0, dim=1)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(X) = \u03bb\u22c5\u2211_i\\|x_i\\|\n\n\n\n\n\nfor a nonnegative \n\u03bb\n, where $x_i$ is the $i$-th column of $X$ if \ndim == 1\n, and the $i$-th row of $X$ if \ndim == 2\n. In words, it is the sum of the Euclidean norms of the columns or rows.\n\n\nsource\n\n\n#\n\n\nProximalOperators.NormLinf\n \n \nFunction\n.\n\n\n$L_\u221e$ norm\n\n\nNormLinf(\u03bb=1.0)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(x) = \u03bb\u22c5\\max\\{|x_1|, \u2026, |x_n|\\},\n\n\n\n\n\nfor a nonnegative parameter \n\u03bb\n.\n\n\nsource\n\n\n#\n\n\nProximalOperators.NuclearNorm\n \n \nType\n.\n\n\nNuclear norm\n\n\nNuclearNorm(\u03bb=1.0)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(X) = \\|X\\|_* = \u03bb \u2211_i \u03c3_i(X),\n\n\n\n\n\nwhere \n\u03bb\n is a positive parameter and $\u03c3_i(X)$ is $i$-th singular value of matrix $X$.\n\n\nsource\n\n\n#\n\n\nProximalOperators.SqrNormL2\n \n \nType\n.\n\n\nSquared Euclidean norm (weighted)\n\n\nSqrNormL2(\u03bb=1.0)\n\n\n\n\nWith a nonnegative scalar \n\u03bb\n, returns the function\n\n\n\n\n\nf(x) = \\tfrac{\u03bb}{2}\\|x\\|^2.\n\n\n\n\n\nWith a nonnegative array \n\u03bb\n, returns the function\n\n\n\n\n\nf(x) = \\tfrac{1}{2}\u2211_i \u03bb_i x_i^2.\n\n\n\n\n\nsource\n\n\n\n\nPenalties and other functions\n\n\n#\n\n\nProximalOperators.HingeLoss\n \n \nFunction\n.\n\n\nHinge loss\n\n\nHingeLoss(b, \u03bc=1.0)\n\n\nReturns the function\n\n\n\n\n\nf(x) = \u03bc\u22c5\u2211_i \\max\\{0, 1 - b_i * x_i\\},\n\n\n\n\n\nwhere \nb\n is an array and \n\u03bc\n is a positive parameter.\n\n\nsource\n\n\n#\n\n\nProximalOperators.HuberLoss\n \n \nType\n.\n\n\nHuber loss\n\n\nHuberLoss(\u03c1=1.0, \u03bc=1.0)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(x) = \\begin{cases}\n  \\tfrac{\u03bc}{2}\\|x\\|^2 & \\text{if}\\ \\|x\\| \u2a7d \u03c1 \\\\\n  \u03c1\u03bc(\\|x\\| - \\tfrac{\u03c1}{2}) & \\text{otherwise},\n\\end{cases}\n\n\n\n\n\nwhere \n\u03c1\n and \n\u03bc\n are positive parameters.\n\n\nsource\n\n\n#\n\n\nProximalOperators.LeastSquares\n \n \nType\n.\n\n\nLeast squares penalty\n\n\nLeastSquares(A, b, \u03bb=1.0)\n\n\n\n\nFor a matrix \nA\n, a vector \nb\n and a scalar \n\u03bb\n, returns the function\n\n\n\n\n\nf(x) = \\tfrac{\\lambda}{2}\\|Ax - b\\|^2.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.Linear\n \n \nType\n.\n\n\nLinear function\n\n\nLinear(c)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(x) = \\langle c, x \\rangle.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.LogBarrier\n \n \nType\n.\n\n\nLogarithmic barrier\n\n\nLogBarrier(a=1.0, b=0.0, \u03bc=1.0)\n\n\n\n\nReturns the function\n\n\n\n\n\nf(x) = -\u03bc\u22c5\u2211_i\\log(a\u22c5x_i+b),\n\n\n\n\n\nfor a nonnegative parameter \n\u03bc\n.\n\n\nsource\n\n\n#\n\n\nProximalOperators.Maximum\n \n \nFunction\n.\n\n\nMaximum coefficient\n\n\nMaximum(\u03bb=1.0)\n\n\n\n\nFor a nonnegative parameter \n\u03bb \u2a7e 0\n, returns the function\n\n\n\n\n\nf(x) = \\lambda \\cdot \\max \\{x_i : i = 1,\\ldots, n \\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.Quadratic\n \n \nType\n.\n\n\nQuadratic function\n\n\nQuadratic(Q, q)\n\n\n\n\nFor a matrix \nQ\n (dense or sparse, symmetric and positive definite) and a vector \nq\n, returns the function\n\n\n\n\n\nf(x) = \\tfrac{1}{2}\\langle Qx, x\\rangle + \\langle q, x \\rangle.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.QuadraticIterative\n \n \nType\n.\n\n\nQuadratic function (iterative evaluation of prox)\n\n\nQuadraticIterative(Q, q)\n\n\n\n\nFor a matrix \nQ\n (dense or sparse, symmetric and positive definite) and a vector \nq\n, returns the function\n\n\n\n\n\nf(x) = \\tfrac{1}{2}\\langle Qx, x\\rangle + \\langle q, x \\rangle.\n\n\n\n\n\nDifferently from \nQuadratic\n, in this case the \nprox\n operation is evaluated (inexactly) using an iterative method.\n\n\nsource\n\n\n#\n\n\nProximalOperators.SumPositive\n \n \nType\n.\n\n\nSum of the positive coefficients\n\n\nSumPositive()\n\n\n\n\nReturns the function\n\n\n\n\n\nf(x) = \u2211_i \\max\\{0, x_i\\}.\n\n\n\n\n\nsource", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#functions", 
            "text": "Here we list the available functions, grouped by category. Each function is documented with its exact definition and the necessary parameters for construction. The proximal mapping (and gradient, when defined) of such functions is computed by calling the  prox  and  prox!  methods (and  gradient ,  gradient! , when defined). These functions can be modified and/or combined together to make new ones, by means of  calculus rules .", 
            "title": "Functions"
        }, 
        {
            "location": "/functions/#indicators-of-sets", 
            "text": "When function  f  is the indicator function of a set  S , that is   \nf(x) = \u03b4_S(x) =\n\\begin{cases}\n0 & \\text{if}\\ x \\in S, \\\\\n+\u221e & \\text{otherwise},\n\\end{cases}   then  \\mathrm{prox}_{\u03b3f} = \u03a0_S  is the projection onto  S . Therefore ProximalOperators includes in particular projections onto commonly used sets, which are here listed.  #  ProximalOperators.IndAffine     Type .  Indicator of an affine subspace  IndAffine(A, b)  If  A  is a matrix (dense or sparse) and  b  is a vector, returns the indicator function of the set   \nS = \\{x : Ax = b\\}.   If  A  is a vector and  b  is a scalar, returns the indicator function of the set   \nS = \\{x : \\langle A, x \\rangle = b\\}.   source  #  ProximalOperators.IndBallLinf     Function .  Indicator of a $L_\u221e$ norm ball  IndBallLinf(r=1.0)  Returns the indicator function of the set   \nS = \\{ x : \\max (|x_i|) \\leq r \\}.   Parameter  r  must be positive.  source  #  ProximalOperators.IndBallL0     Type .  Indicator of a $L_0$ pseudo-norm ball  IndBallL0(r=1)  Returns the indicator function of the set   \nS = \\{ x : \\mathrm{nnz}(x) \\leq r \\}.   Parameter  r  must be a positive integer.  source  #  ProximalOperators.IndBallL1     Type .  Indicator of a $L_1$ norm ball  IndBallL1(r=1.0)  Returns the indicator function of the set   \nS = \\left\\{ x : \u2211_i |x_i| \\leq r \\right\\}.   Parameter  r  must be positive.  source  #  ProximalOperators.IndBallL2     Type .  Indicator of a Euclidean ball  IndBallL2(r=1.0)  Returns the indicator function of the set   \nS = \\{ x : \\|x\\| \\leq r \\},   where $|\\cdot|$ is the $L_2$ (Euclidean) norm. Parameter  r  must be positive.  source  #  ProximalOperators.IndBallRank     Type .  Indicator of rank ball  IndBallRank(r=1)  Returns the indicator function of the set of matrices of rank at most  r :   \nS = \\{ X : \\mathrm{rank}(X) \\leq r \\},   Parameter  r  must be a positive integer.  source  #  ProximalOperators.IndBinary     Type .  Indicator of the product of binary sets  IndBinary(low, up)  Returns the indicator function of the set   \nS = \\{ x : x_i = low_i\\ \\text{or}\\ x_i = up_i \\},   Parameters  low  and  up  can be either scalars or arrays of the same dimension as the space.  source  #  ProximalOperators.IndBox     Type .  Indicator of a box  IndBox(low, up)  Returns the indicator function of the set   \nS = \\{ x : low \\leq x \\leq up \\}.   Parameters  low  and  up  can be either scalars or arrays of the same dimension as the space: they must satisfy  low  = up , and are allowed to take values  -Inf  and  +Inf  to indicate unbounded coordinates.  source  #  ProximalOperators.IndHalfspace     Type .  Indicator of a halfspace  IndHalfspace(a, b)  For an array  a  and a scalar  b , returns the indicator of set   \nS = \\{x : \\langle a,x \\rangle \\leq b \\}.   source  #  ProximalOperators.IndPoint     Type .  Indicator of a singleton  IndPoint(p=0.0)  Returns the indicator of the set   \nC = \\{p \\}.   Parameter  p  can be a scalar, in which case the unique element of  S  has uniform coefficients.  source  #  ProximalOperators.IndSimplex     Type .  Indicator of a simplex  IndSimplex(a=1.0)  Returns the indicator of the set   \nS = \\left\\{ x : x \\geq 0, \u2211_i x_i = a \\right\\}.   By default  a=1.0 , therefore $S$ is the probability simplex.  source  #  ProximalOperators.IndSphereL2     Type .  Indicator of a Euclidean sphere  IndSphereL2(r=1.0)  Returns the indicator function of the set   \nS = \\{ x : \\|x\\| = r \\},   where $|cdot|$ is the $L_2$ (Euclidean) norm. Parameter  r  must be positive.  source", 
            "title": "Indicators of sets"
        }, 
        {
            "location": "/functions/#indicators-of-convex-cones", 
            "text": "An important class of sets in optimization is that of convex cones. These are used in particular for formulating  cone programming problems , a family of problems which includes linear programs (LP), quadratic programs (QP), quadratically constrained quadratic programs (QCQP) and semidefinite programs (SDP).  #  ProximalOperators.IndExpPrimal     Type .  Indicator of the (primal) exponential cone  IndExpPrimal()  Returns the indicator function of the primal exponential cone, that is   \nC = \\mathrm{cl} \\{ (r,s,t) : s > 0, s\u22c5e^{r/s} \\leq t \\} \\subset \\mathbb{R}^3.   source  #  ProximalOperators.IndExpDual     Function .  Indicator of the (dual) exponential cone  IndExpDual()  Returns the indicator function of the dual exponential cone, that is   \nC = \\mathrm{cl} \\{ (u,v,w) : u < 0, -u\u22c5e^{v/u} \\leq w\u22c5e \\} \\subset \\mathbb{R}^3.   source  #  ProximalOperators.IndFree     Type .  Indicator of the free cone  IndFree()  Returns the indicator function of the whole space, or \"free cone\",  i.e. , a function which is identically zero.  source  #  ProximalOperators.IndNonnegative     Type .  Indicator of the nonnegative orthant  IndNonnegative()  Returns the indicator of the set   \nC = \\{ x : x \\geq 0 \\}.   source  #  ProximalOperators.IndNonpositive     Type .  Indicator of the nonpositive orthant  IndNonpositive()  Returns the indicator of the set   \nC = \\{ x : x \\leq 0 \\}.   source  #  ProximalOperators.IndPSD     Type .  Indicator of the set of positive semi-definite cone  IndPSD()  Returns the indicator of the set   \nC = \\{ X : X \\succeq 0 \\}.   The argument to the function can be either a  Symmetric  or  Hermitian  object, or an object of type  AbstractVector{Float64}  holding a symmetric matrix in (lower triangular) packed storage.  source  #  ProximalOperators.IndSOC     Type .  Indicator of the second-order cone  IndSOC()  Returns the indicator of the second-order cone (also known as ice-cream cone or Lorentz cone), that is   \nC = \\left\\{ (t, x) : \\|x\\| \\leq t \\right\\}.   source  #  ProximalOperators.IndRotatedSOC     Type .  Indicator of the rotated second-order cone  IndRotatedSOC()  Returns the indicator of the  rotated  second-order cone (also known as ice-cream cone or Lorentz cone), that is   \nC = \\left\\{ (p, q, x) : \\|x\\|^2 \\leq 2\\cdot pq, p \\geq 0, q \\geq 0 \\right\\}.   source  #  ProximalOperators.IndZero     Type .  Indicator of the zero cone  IndZero()  Returns the indicator function of the set containing the origin, the \"zero cone\".  source", 
            "title": "Indicators of convex cones"
        }, 
        {
            "location": "/functions/#norms-and-regularization-functions", 
            "text": "#  ProximalOperators.ElasticNet     Type .  Elastic-net regularization  ElasticNet(\u03bc=1.0, \u03bb=1.0)  Returns the function   \nf(x) = \u03bc\\|x\\|_1 + (\u03bb/2)\\|x\\|^2,   for nonnegative parameters  \u03bc  and  \u03bb .  source  #  ProximalOperators.NormL0     Type .  $L_0$ pseudo-norm  NormL0(\u03bb=1.0)  Returns the function   \nf(x) = \u03bb\\cdot\\mathrm{nnz}(x)   for a nonnegative parameter  \u03bb .  source  #  ProximalOperators.NormL1     Type .  $L_1$ norm  NormL1(\u03bb=1.0)  With a nonnegative scalar parameter \u03bb, returns the function   \nf(x) = \u03bb\\cdot\u2211_i|x_i|.   With a nonnegative array parameter \u03bb, returns the function   \nf(x) = \u2211_i \u03bb_i|x_i|.   source  #  ProximalOperators.NormL2     Type .  $L_2$ norm  NormL2(\u03bb=1.0)  With a nonnegative scalar parameter \u03bb, returns the function   \nf(x) = \u03bb\\cdot\\sqrt{x_1^2 + \u2026 + x_n^2}.   source  #  ProximalOperators.NormL21     Type .  Sum-of-$L_2$ norms  NormL21(\u03bb=1.0, dim=1)  Returns the function   \nf(X) = \u03bb\u22c5\u2211_i\\|x_i\\|   for a nonnegative  \u03bb , where $x_i$ is the $i$-th column of $X$ if  dim == 1 , and the $i$-th row of $X$ if  dim == 2 . In words, it is the sum of the Euclidean norms of the columns or rows.  source  #  ProximalOperators.NormLinf     Function .  $L_\u221e$ norm  NormLinf(\u03bb=1.0)  Returns the function   \nf(x) = \u03bb\u22c5\\max\\{|x_1|, \u2026, |x_n|\\},   for a nonnegative parameter  \u03bb .  source  #  ProximalOperators.NuclearNorm     Type .  Nuclear norm  NuclearNorm(\u03bb=1.0)  Returns the function   \nf(X) = \\|X\\|_* = \u03bb \u2211_i \u03c3_i(X),   where  \u03bb  is a positive parameter and $\u03c3_i(X)$ is $i$-th singular value of matrix $X$.  source  #  ProximalOperators.SqrNormL2     Type .  Squared Euclidean norm (weighted)  SqrNormL2(\u03bb=1.0)  With a nonnegative scalar  \u03bb , returns the function   \nf(x) = \\tfrac{\u03bb}{2}\\|x\\|^2.   With a nonnegative array  \u03bb , returns the function   \nf(x) = \\tfrac{1}{2}\u2211_i \u03bb_i x_i^2.   source", 
            "title": "Norms and regularization functions"
        }, 
        {
            "location": "/functions/#penalties-and-other-functions", 
            "text": "#  ProximalOperators.HingeLoss     Function .  Hinge loss  HingeLoss(b, \u03bc=1.0)  Returns the function   \nf(x) = \u03bc\u22c5\u2211_i \\max\\{0, 1 - b_i * x_i\\},   where  b  is an array and  \u03bc  is a positive parameter.  source  #  ProximalOperators.HuberLoss     Type .  Huber loss  HuberLoss(\u03c1=1.0, \u03bc=1.0)  Returns the function   \nf(x) = \\begin{cases}\n  \\tfrac{\u03bc}{2}\\|x\\|^2 & \\text{if}\\ \\|x\\| \u2a7d \u03c1 \\\\\n  \u03c1\u03bc(\\|x\\| - \\tfrac{\u03c1}{2}) & \\text{otherwise},\n\\end{cases}   where  \u03c1  and  \u03bc  are positive parameters.  source  #  ProximalOperators.LeastSquares     Type .  Least squares penalty  LeastSquares(A, b, \u03bb=1.0)  For a matrix  A , a vector  b  and a scalar  \u03bb , returns the function   \nf(x) = \\tfrac{\\lambda}{2}\\|Ax - b\\|^2.   source  #  ProximalOperators.Linear     Type .  Linear function  Linear(c)  Returns the function   \nf(x) = \\langle c, x \\rangle.   source  #  ProximalOperators.LogBarrier     Type .  Logarithmic barrier  LogBarrier(a=1.0, b=0.0, \u03bc=1.0)  Returns the function   \nf(x) = -\u03bc\u22c5\u2211_i\\log(a\u22c5x_i+b),   for a nonnegative parameter  \u03bc .  source  #  ProximalOperators.Maximum     Function .  Maximum coefficient  Maximum(\u03bb=1.0)  For a nonnegative parameter  \u03bb \u2a7e 0 , returns the function   \nf(x) = \\lambda \\cdot \\max \\{x_i : i = 1,\\ldots, n \\}.   source  #  ProximalOperators.Quadratic     Type .  Quadratic function  Quadratic(Q, q)  For a matrix  Q  (dense or sparse, symmetric and positive definite) and a vector  q , returns the function   \nf(x) = \\tfrac{1}{2}\\langle Qx, x\\rangle + \\langle q, x \\rangle.   source  #  ProximalOperators.QuadraticIterative     Type .  Quadratic function (iterative evaluation of prox)  QuadraticIterative(Q, q)  For a matrix  Q  (dense or sparse, symmetric and positive definite) and a vector  q , returns the function   \nf(x) = \\tfrac{1}{2}\\langle Qx, x\\rangle + \\langle q, x \\rangle.   Differently from  Quadratic , in this case the  prox  operation is evaluated (inexactly) using an iterative method.  source  #  ProximalOperators.SumPositive     Type .  Sum of the positive coefficients  SumPositive()  Returns the function   \nf(x) = \u2211_i \\max\\{0, x_i\\}.   source", 
            "title": "Penalties and other functions"
        }, 
        {
            "location": "/calculus/", 
            "text": "Calculus rules\n\n\nThe calculus rules described in the following allow to modify and combine \nfunctions\n, to obtain new ones with efficiently computable proximal mapping.\n\n\n\n\nDuality\n\n\n#\n\n\nProximalOperators.Conjugate\n \n \nType\n.\n\n\nConvex conjugate\n\n\nConjugate(f)\n\n\n\n\nReturns the convex conjugate (also known as Fenchel conjugate, or Fenchel-Legendre transform) of function \nf\n, that is\n\n\n\n\n\nf^*(x) = \\sup_y \\{ \\langle y, x \\rangle - f(y) \\}.\n\n\n\n\n\nsource\n\n\n\n\nDistances from convex sets\n\n\nWhen the indicator of a convex set is constructed (see \nIndicators of sets\n) the (squared) distance from the set can be constructed using the following:\n\n\n#\n\n\nProximalOperators.DistL2\n \n \nType\n.\n\n\nDistance from a convex set\n\n\nDistL2(ind_S)\n\n\n\n\nGiven \nind_S\n the indicator function of a convex set $S$, and an optional positive parameter \n\u03bb\n, returns the (weighted) Euclidean distance from $S$, that is function\n\n\n\n\n\ng(x) = \\mathrm{dist}_S(x) = \\min \\{ \\|y - x\\| : y \\in S \\}.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.SqrDistL2\n \n \nType\n.\n\n\nSquared distance from a convex set\n\n\nSqrDistL2(ind_S, \u03bb=1.0)\n\n\n\n\nGiven \nind_S\n the indicator function of a convex set $S$, and an optional positive parameter \n\u03bb\n, returns the (weighted) squared Euclidean distance from $S$, that is function\n\n\n\n\n\ng(x) = \\tfrac{\u03bb}{2}\\mathrm{dist}_S^2(x) = \\min \\left\\{ \\tfrac{\u03bb}{2}\\|y - x\\|^2 : y \\in S \\right\\}.\n\n\n\n\n\nsource\n\n\n\n\nFunctions combination\n\n\nThe following means of combination are important in that they allow to represent a very common situation: defining the sum of multiple functions, each applied to an independent block of variables. The following two constructors, \nSeparableSum\n and \nSlicedSeparableSum\n, allow to do this in two (complementary) ways.\n\n\n#\n\n\nProximalOperators.SeparableSum\n \n \nType\n.\n\n\nSeparable sum of functions\n\n\nSeparableSum(f\u2081,\u2026,f\u2096)\n\n\n\n\nGiven functions \nf\u2081\n to \nf\u2096\n, returns their separable sum, that is\n\n\n\n\n\ng(x_1,\u2026,x_k) = \u2211_{i=1}^k f_i(x_i).\n\n\n\n\n\nThe object \ng\n constructed in this way can be evaluated at \nTuple\ns of length \nk\n. Likewise, the \nprox\n and \nprox!\n methods for \ng\n operate with (input and output) \nTuple\ns of length \nk\n.\n\n\nExample:\n\n\nf = SeparableSum(NormL1(), NuclearNorm()); # separable sum of two functions\nx = randn(10); # some random vector\nY = randn(20, 30); # some random matrix\nf_xY = f((x, Y)); # evaluates f at (x, Y)\n(u, V), f_uV = prox(f, (x, Y), 1.3); # computes prox at (x, Y)\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.SlicedSeparableSum\n \n \nType\n.\n\n\nSliced separable sum of functions\n\n\nSlicedSeparableSum((f\u2081,\u2026,f\u2096), (J\u2081,\u2026,J\u2096))\n\n\n\n\nReturns the function\n\n\n\n\n\ng(x) = \u2211_{i=1}^k f_i(x_{J_i}).\n\n\n\n\n\nSlicedSeparableSum(f, (J\u2081,\u2026,J\u2096))\n\n\n\n\nAnalogous to the previous one, but applies the same function \nf\n to all slices of the variable \nx\n:\n\n\n\n\n\ng(x) = \u2211_{i=1}^k f(x_{J_i}).\n\n\n\n\n\nsource\n\n\n\n\nFunctions regularization\n\n\n#\n\n\nProximalOperators.MoreauEnvelope\n \n \nType\n.\n\n\nMoreau envelope\n\n\nMoreauEnvelope(f, \u03b3=1.0)\n\n\n\n\nReturns the Moreau envelope (also known as Moreau-Yosida regularization) of function \nf\n with parameter \n\u03b3\n (positive), that is\n\n\n\n\n\nf^\u03b3(x) = \\min_z \\left\\{ f(z) + \\tfrac{1}{2\u03b3}\\|z-x\\|^2 \\right\\}.\n\n\n\n\n\nIf $f$ is convex, then $f^\u03b3$ is a smooth, convex, lower approximation to $f$, having the same minima as the original function.\n\n\nsource\n\n\n#\n\n\nProximalOperators.Regularize\n \n \nType\n.\n\n\nRegularize\n\n\nRegularize(f, \u03c1=1.0, a=0.0)\n\n\n\n\nGiven function \nf\n, and optional parameters \n\u03c1\n (positive) and \na\n, returns\n\n\n\n\n\ng(x) = f(x) + \\tfrac{\u03c1}{2}\\|x-a\\|\u00b2.\n\n\n\n\n\nParameter \na\n can be either an array or a scalar, in which case it is subtracted component-wise from \nx\n in the above expression.\n\n\nsource\n\n\n\n\nPre- and post-transformations\n\n\n#\n\n\nProximalOperators.Postcompose\n \n \nType\n.\n\n\nPostcomposition with an affine transformation\n\n\nPostcompose(f, a=1.0, b=0.0)\n\n\n\n\nReturns the function\n\n\n\n\n\ng(x) = a\\cdot f(x) + b.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.Precompose\n \n \nType\n.\n\n\nPrecomposition with linear mapping/translation\n\n\nPrecompose(f, L, \u03bc, b)\n\n\n\n\nReturns the function\n\n\n\n\n\ng(x) = f(Lx + b)\n\n\n\n\n\nwhere $f$ is a convex function and $L$ is a linear mapping: this must satisfy $LL^* = \u03bcI$ for $\u03bc \u2a7e 0$. Furthermore, either $f$ is separable or parameter \n\u03bc\n is a scalar, for the \nprox\n of $g$ to be computable.\n\n\nParameter \nL\n defines $L$ through the \nA_mul_B!\n and \nAc_mul_B!\n methods. Therefore \nL\n can be an \nAbstractMatrix\n for example, but not necessarily.\n\n\nIn this case, \nprox\n and \nprox!\n are computed according to Prop. 24.14 in Bauschke, Combettes \"Convex Analisys and Monotone Operator Theory in Hilbert Spaces\", 2nd edition, 2016. The same result is Prop. 23.32 in the 1st edition of the same book.\n\n\nsource\n\n\n#\n\n\nProximalOperators.PrecomposeDiagonal\n \n \nType\n.\n\n\nPrecomposition with diagonal scaling/translation\n\n\nPrecomposeDiagonal(f, a, b)\n\n\n\n\nReturns the function\n\n\n\n\n\ng(x) = f(\\mathrm{diag}(a)x + b)\n\n\n\n\n\nwhere $f$ is a convex function. Furthermore, $f$ must be separable, or \na\n must be a scalar, for the \nprox\n of $g$ to be computable. Parametes \na\n and \nb\n can be arrays of multiple dimensions, according to the shape/size of the input \nx\n that will be provided to the function: the way the above expression for $g$ should be thought of, is \ng(x) = f(a.*x + b)\n.\n\n\nsource\n\n\n#\n\n\nProximalOperators.Tilt\n \n \nType\n.\n\n\nLinear tilting\n\n\nTilt(f, a, b=0.0)\n\n\n\n\nGiven function \nf\n, an array \na\n and a constant \nb\n (optional), returns function\n\n\n\n\n\ng(x) = f(x) + \\langle a, x \\rangle + b.\n\n\n\n\n\nsource\n\n\n#\n\n\nProximalOperators.Translate\n \n \nType\n.\n\n\nTranslation\n\n\nTranslate(f, b)\n\n\n\n\nReturns the translated function\n\n\n\n\n\ng(x) = f(x + b)\n\n\n\n\n\nsource", 
            "title": "Calculus rules"
        }, 
        {
            "location": "/calculus/#calculus-rules", 
            "text": "The calculus rules described in the following allow to modify and combine  functions , to obtain new ones with efficiently computable proximal mapping.", 
            "title": "Calculus rules"
        }, 
        {
            "location": "/calculus/#duality", 
            "text": "#  ProximalOperators.Conjugate     Type .  Convex conjugate  Conjugate(f)  Returns the convex conjugate (also known as Fenchel conjugate, or Fenchel-Legendre transform) of function  f , that is   \nf^*(x) = \\sup_y \\{ \\langle y, x \\rangle - f(y) \\}.   source", 
            "title": "Duality"
        }, 
        {
            "location": "/calculus/#distances-from-convex-sets", 
            "text": "When the indicator of a convex set is constructed (see  Indicators of sets ) the (squared) distance from the set can be constructed using the following:  #  ProximalOperators.DistL2     Type .  Distance from a convex set  DistL2(ind_S)  Given  ind_S  the indicator function of a convex set $S$, and an optional positive parameter  \u03bb , returns the (weighted) Euclidean distance from $S$, that is function   \ng(x) = \\mathrm{dist}_S(x) = \\min \\{ \\|y - x\\| : y \\in S \\}.   source  #  ProximalOperators.SqrDistL2     Type .  Squared distance from a convex set  SqrDistL2(ind_S, \u03bb=1.0)  Given  ind_S  the indicator function of a convex set $S$, and an optional positive parameter  \u03bb , returns the (weighted) squared Euclidean distance from $S$, that is function   \ng(x) = \\tfrac{\u03bb}{2}\\mathrm{dist}_S^2(x) = \\min \\left\\{ \\tfrac{\u03bb}{2}\\|y - x\\|^2 : y \\in S \\right\\}.   source", 
            "title": "Distances from convex sets"
        }, 
        {
            "location": "/calculus/#functions-combination", 
            "text": "The following means of combination are important in that they allow to represent a very common situation: defining the sum of multiple functions, each applied to an independent block of variables. The following two constructors,  SeparableSum  and  SlicedSeparableSum , allow to do this in two (complementary) ways.  #  ProximalOperators.SeparableSum     Type .  Separable sum of functions  SeparableSum(f\u2081,\u2026,f\u2096)  Given functions  f\u2081  to  f\u2096 , returns their separable sum, that is   \ng(x_1,\u2026,x_k) = \u2211_{i=1}^k f_i(x_i).   The object  g  constructed in this way can be evaluated at  Tuple s of length  k . Likewise, the  prox  and  prox!  methods for  g  operate with (input and output)  Tuple s of length  k .  Example:  f = SeparableSum(NormL1(), NuclearNorm()); # separable sum of two functions\nx = randn(10); # some random vector\nY = randn(20, 30); # some random matrix\nf_xY = f((x, Y)); # evaluates f at (x, Y)\n(u, V), f_uV = prox(f, (x, Y), 1.3); # computes prox at (x, Y)  source  #  ProximalOperators.SlicedSeparableSum     Type .  Sliced separable sum of functions  SlicedSeparableSum((f\u2081,\u2026,f\u2096), (J\u2081,\u2026,J\u2096))  Returns the function   \ng(x) = \u2211_{i=1}^k f_i(x_{J_i}).   SlicedSeparableSum(f, (J\u2081,\u2026,J\u2096))  Analogous to the previous one, but applies the same function  f  to all slices of the variable  x :   \ng(x) = \u2211_{i=1}^k f(x_{J_i}).   source", 
            "title": "Functions combination"
        }, 
        {
            "location": "/calculus/#functions-regularization", 
            "text": "#  ProximalOperators.MoreauEnvelope     Type .  Moreau envelope  MoreauEnvelope(f, \u03b3=1.0)  Returns the Moreau envelope (also known as Moreau-Yosida regularization) of function  f  with parameter  \u03b3  (positive), that is   \nf^\u03b3(x) = \\min_z \\left\\{ f(z) + \\tfrac{1}{2\u03b3}\\|z-x\\|^2 \\right\\}.   If $f$ is convex, then $f^\u03b3$ is a smooth, convex, lower approximation to $f$, having the same minima as the original function.  source  #  ProximalOperators.Regularize     Type .  Regularize  Regularize(f, \u03c1=1.0, a=0.0)  Given function  f , and optional parameters  \u03c1  (positive) and  a , returns   \ng(x) = f(x) + \\tfrac{\u03c1}{2}\\|x-a\\|\u00b2.   Parameter  a  can be either an array or a scalar, in which case it is subtracted component-wise from  x  in the above expression.  source", 
            "title": "Functions regularization"
        }, 
        {
            "location": "/calculus/#pre-and-post-transformations", 
            "text": "#  ProximalOperators.Postcompose     Type .  Postcomposition with an affine transformation  Postcompose(f, a=1.0, b=0.0)  Returns the function   \ng(x) = a\\cdot f(x) + b.   source  #  ProximalOperators.Precompose     Type .  Precomposition with linear mapping/translation  Precompose(f, L, \u03bc, b)  Returns the function   \ng(x) = f(Lx + b)   where $f$ is a convex function and $L$ is a linear mapping: this must satisfy $LL^* = \u03bcI$ for $\u03bc \u2a7e 0$. Furthermore, either $f$ is separable or parameter  \u03bc  is a scalar, for the  prox  of $g$ to be computable.  Parameter  L  defines $L$ through the  A_mul_B!  and  Ac_mul_B!  methods. Therefore  L  can be an  AbstractMatrix  for example, but not necessarily.  In this case,  prox  and  prox!  are computed according to Prop. 24.14 in Bauschke, Combettes \"Convex Analisys and Monotone Operator Theory in Hilbert Spaces\", 2nd edition, 2016. The same result is Prop. 23.32 in the 1st edition of the same book.  source  #  ProximalOperators.PrecomposeDiagonal     Type .  Precomposition with diagonal scaling/translation  PrecomposeDiagonal(f, a, b)  Returns the function   \ng(x) = f(\\mathrm{diag}(a)x + b)   where $f$ is a convex function. Furthermore, $f$ must be separable, or  a  must be a scalar, for the  prox  of $g$ to be computable. Parametes  a  and  b  can be arrays of multiple dimensions, according to the shape/size of the input  x  that will be provided to the function: the way the above expression for $g$ should be thought of, is  g(x) = f(a.*x + b) .  source  #  ProximalOperators.Tilt     Type .  Linear tilting  Tilt(f, a, b=0.0)  Given function  f , an array  a  and a constant  b  (optional), returns function   \ng(x) = f(x) + \\langle a, x \\rangle + b.   source  #  ProximalOperators.Translate     Type .  Translation  Translate(f, b)  Returns the translated function   \ng(x) = f(x + b)   source", 
            "title": "Pre- and post-transformations"
        }, 
        {
            "location": "/operators/", 
            "text": "Prox and gradient\n\n\nThe following methods allow to evaluate the proximal mapping (and gradient, when defined) of mathematical functions, which are constructed according to what described in \nFunctions\n and \nCalculus rules\n.\n\n\n#\n\n\nProximalOperators.prox\n \n \nFunction\n.\n\n\nProximal mapping\n\n\nprox(f, x, \u03b3=1.0)\n\n\n\n\nComputes\n\n\n\n\n\ny = \\mathrm{prox}_{\\gamma f}(x) = \\arg\\min_z \\left\\{ f(z) + \\tfrac{1}{2\\gamma}\\|z-x\\|^2 \\right\\}.\n\n\n\n\n\nThe resulting point $y$ is returned as first output, and $f(y)$ as second output.\n\n\nsource\n\n\n#\n\n\nProximalOperators.prox!\n \n \nFunction\n.\n\n\nProximal mapping (in-place)\n\n\nprox!(y, f, x, \u03b3=1.0)\n\n\n\n\nComputes\n\n\n\n\n\ny = \\mathrm{prox}_{\\gamma f}(x) = \\arg\\min_z \\left\\{ f(z) + \\tfrac{1}{2\\gamma}\\|z-x\\|^2 \\right\\}.\n\n\n\n\n\nThe resulting point $y$ is written to the (pre-allocated) array \ny\n, which must have the same shape/size as \nx\n, and the value the proximal point of $x$ with respect to function $f(y)$ is returned.\n\n\nsource\n\n\n#\n\n\nBase.LinAlg.gradient\n \n \nFunction\n.\n\n\nGradient mapping\n\n\ngradient(f, x)\n\n\n\n\nFor a differentiable function $f$, returns $\\nabla f(x)$ as first output, and $f(x)$ as second output.\n\n\nsource\n\n\n#\n\n\nProximalOperators.gradient!\n \n \nFunction\n.\n\n\nGradient mapping (in-place)\n\n\ngradient!(y, f, x)\n\n\n\n\nFor a differentiable function $f$, writes $\\nabla f(x)$ to \ny\n, which must be pre-allocated and have the same shape/size as \nx\n, and returns $f(x)$ as output.\n\n\nsource\n\n\n\n\nComplex and matrix variables\n\n\nThe proximal mapping is usually discussed in the case of functions over \n\\mathbb{R}^n\n. However, by adapting the inner product \n\\langle\\cdot,\\cdot\\rangle\n and associated norm \n\\|\\cdot\\|\n adopted in its definition, one can extend the concept to functions over more general spaces. When functions of unidimensional arrays (vectors) are concerned, the standard Euclidean product and norm are used in defining \nprox\n (therefore \nprox!\n, but also \ngradient\n and \ngradient!\n). This are the inner product and norm which are computed by \ndot\n and \nnorm\n in Julia.\n\n\nWhen bidimensional, tridimensional (matrices and tensors) and higher dimensional arrays are concerned, then the definitions of proximal mapping and gradient are naturally extended by considering the appropriate inner product. For \nk\n-dimensional arrays, of size \nn_1 \\times n_2 \\times \\ldots \\times n_k\n, we consider the inner product\n\n\n\n\n\n\\langle A, B \\rangle = \\sum_{i_1,\\ldots,i_k} A_{i_1,\\ldots,i_k} \\cdot B_{i_1,\\ldots,i_k}\n\n\n\n\n\nwhich reduces to the usual Euclidean product in case of unidimensional arrays, and to the \ntrace product\n \n\\langle A, B \\rangle = \\mathrm{tr}(A^\\top B)\n in the case of matrices (bidimensional arrays). This inner product, and the associated norm, are the ones computed by \nvecdot\n and \nvecnorm\n in Julia.\n\n\n\n\nMultiple variable blocks\n\n\nBy combining functions together through \nSeparableSum\n, the resulting function will have multiple inputs, \ni.e.\n, it will be defined over the \nCartesian product\n of the domains of the individual functions. To represent elements (points) of such product space, here we use Julia's \nTuple\n objects.\n\n\nExample.\n Suppose that the following function needs to be represented:\n\n\n\n\n\nf(x, Y) = \\|x\\|_1 + \\|Y\\|_*,\n\n\n\n\n\nthat is, the sum of the \nL_1\n norm of some vector \nx\n and the nuclear norm (the sum of the singular values) of some matrix \nY\n. This is accomplished as follows. Function \nf\n will be defined over \npairs\n of appropriate \nArray\n objects. Likewise, the \nprox\n method will take pairs of \nArray\ns as inputs, and return pairs of \nArray\ns as output:\n\n\nusing ProximalOperators\nf = SeparableSum(NormL1(), NuclearNorm());\nx = randn(10); # some random vector\nY = randn(20, 30); # some random matrix\nf_xY = f((x, Y)); # evaluates f at (x, Y)\n(u, V), f_uV = prox(f, (x, Y), 1.3); # computes prox at (x, Y)\n\n\n\n\nThe same holds for the separable sum of more than two functions, in which case \"pairs\" are to be replaced with \nTuple\ns of the appropriate length.", 
            "title": "Prox and gradient"
        }, 
        {
            "location": "/operators/#prox-and-gradient", 
            "text": "The following methods allow to evaluate the proximal mapping (and gradient, when defined) of mathematical functions, which are constructed according to what described in  Functions  and  Calculus rules .  #  ProximalOperators.prox     Function .  Proximal mapping  prox(f, x, \u03b3=1.0)  Computes   \ny = \\mathrm{prox}_{\\gamma f}(x) = \\arg\\min_z \\left\\{ f(z) + \\tfrac{1}{2\\gamma}\\|z-x\\|^2 \\right\\}.   The resulting point $y$ is returned as first output, and $f(y)$ as second output.  source  #  ProximalOperators.prox!     Function .  Proximal mapping (in-place)  prox!(y, f, x, \u03b3=1.0)  Computes   \ny = \\mathrm{prox}_{\\gamma f}(x) = \\arg\\min_z \\left\\{ f(z) + \\tfrac{1}{2\\gamma}\\|z-x\\|^2 \\right\\}.   The resulting point $y$ is written to the (pre-allocated) array  y , which must have the same shape/size as  x , and the value the proximal point of $x$ with respect to function $f(y)$ is returned.  source  #  Base.LinAlg.gradient     Function .  Gradient mapping  gradient(f, x)  For a differentiable function $f$, returns $\\nabla f(x)$ as first output, and $f(x)$ as second output.  source  #  ProximalOperators.gradient!     Function .  Gradient mapping (in-place)  gradient!(y, f, x)  For a differentiable function $f$, writes $\\nabla f(x)$ to  y , which must be pre-allocated and have the same shape/size as  x , and returns $f(x)$ as output.  source", 
            "title": "Prox and gradient"
        }, 
        {
            "location": "/operators/#complex-and-matrix-variables", 
            "text": "The proximal mapping is usually discussed in the case of functions over  \\mathbb{R}^n . However, by adapting the inner product  \\langle\\cdot,\\cdot\\rangle  and associated norm  \\|\\cdot\\|  adopted in its definition, one can extend the concept to functions over more general spaces. When functions of unidimensional arrays (vectors) are concerned, the standard Euclidean product and norm are used in defining  prox  (therefore  prox! , but also  gradient  and  gradient! ). This are the inner product and norm which are computed by  dot  and  norm  in Julia.  When bidimensional, tridimensional (matrices and tensors) and higher dimensional arrays are concerned, then the definitions of proximal mapping and gradient are naturally extended by considering the appropriate inner product. For  k -dimensional arrays, of size  n_1 \\times n_2 \\times \\ldots \\times n_k , we consider the inner product   \n\\langle A, B \\rangle = \\sum_{i_1,\\ldots,i_k} A_{i_1,\\ldots,i_k} \\cdot B_{i_1,\\ldots,i_k}   which reduces to the usual Euclidean product in case of unidimensional arrays, and to the  trace product   \\langle A, B \\rangle = \\mathrm{tr}(A^\\top B)  in the case of matrices (bidimensional arrays). This inner product, and the associated norm, are the ones computed by  vecdot  and  vecnorm  in Julia.", 
            "title": "Complex and matrix variables"
        }, 
        {
            "location": "/operators/#multiple-variable-blocks", 
            "text": "By combining functions together through  SeparableSum , the resulting function will have multiple inputs,  i.e. , it will be defined over the  Cartesian product  of the domains of the individual functions. To represent elements (points) of such product space, here we use Julia's  Tuple  objects.  Example.  Suppose that the following function needs to be represented:   \nf(x, Y) = \\|x\\|_1 + \\|Y\\|_*,   that is, the sum of the  L_1  norm of some vector  x  and the nuclear norm (the sum of the singular values) of some matrix  Y . This is accomplished as follows. Function  f  will be defined over  pairs  of appropriate  Array  objects. Likewise, the  prox  method will take pairs of  Array s as inputs, and return pairs of  Array s as output:  using ProximalOperators\nf = SeparableSum(NormL1(), NuclearNorm());\nx = randn(10); # some random vector\nY = randn(20, 30); # some random matrix\nf_xY = f((x, Y)); # evaluates f at (x, Y)\n(u, V), f_uV = prox(f, (x, Y), 1.3); # computes prox at (x, Y)  The same holds for the separable sum of more than two functions, in which case \"pairs\" are to be replaced with  Tuple s of the appropriate length.", 
            "title": "Multiple variable blocks"
        }, 
        {
            "location": "/demos/", 
            "text": "Demos\n\n\nThe \ndemos folder\n contains examples on how to use the functions of ProximalOperators to implement optimization algorithms. \nWarning:\n Make sure that the version of ProximalOperators that you have installed is up-to-date with the demo script you are trying to run, as the package features may change over time and the \nmaster branch\n be ahead of what you have installed.", 
            "title": "Demos"
        }, 
        {
            "location": "/demos/#demos", 
            "text": "The  demos folder  contains examples on how to use the functions of ProximalOperators to implement optimization algorithms.  Warning:  Make sure that the version of ProximalOperators that you have installed is up-to-date with the demo script you are trying to run, as the package features may change over time and the  master branch  be ahead of what you have installed.", 
            "title": "Demos"
        }
    ]
}